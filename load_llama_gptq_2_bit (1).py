# -*- coding: utf-8 -*-
"""Load llama gptq 2 bit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uqTupx2RNrm1jkzoPLWhU2HouVheDkV2

### https://huggingface.co/acl-srw-2024

https://github.com/AutoGPTQ/AutoGPTQ

https://github.com/Cornell-RelaxML/QuIP

https://github.com/mit-han-lab/llm-awq
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install auto-gptq

# Load model script
from auto_gptq import AutoGPTQForCausalLM
model = AutoGPTQForCausalLM.from_quantized("acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit", device="cuda:0",use_safetensors=True)







# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install auto-gptq

# Load model script
from auto_gptq import AutoGPTQForCausalLM
model = AutoGPTQForCausalLM.from_quantized("acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit", device="cuda:0",use_safetensors=True)

prompt = "What is the capital of France?"  # Replace with your desired prompt
tokens = model.tokenizer(prompt, return_tensors='pt').to(model.device)
generated_ids = model.generate(**tokens)
generated_text = model.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(generated_text)



from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path)
# ... (Rest of your existing code)
tokens = tokenizer(prompt, return_tensors='pt').to(model.device)
generated_ids = model.generate(**tokens)
generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(generated_text)

from auto_gptq import AutoGPTQForCausalLM
from transformers import AutoTokenizer

# Load the model
model = AutoGPTQForCausalLM.from_quantized("acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit", device="cuda:0",use_safetensors=True)

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path)

# Define the prompt
prompt = "who is ai?"  # Your prompt in Arabic

# Tokenize the prompt
tokens = tokenizer(prompt, return_tensors='pt').to(model.device)

# Generate text
generated_ids = model.generate(**tokens)

# Decode the output
generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

# Print the result
print(generated_text)

from auto_gptq import AutoGPTQForCausalLM
from transformers import AutoTokenizer

# Load the model
model = AutoGPTQForCausalLM.from_quantized("acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit", device="cuda:0",use_safetensors=True)

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path)

# Define the prompt
prompt = "who is ai?"  # Your prompt

# Tokenize the prompt
tokens = tokenizer(prompt, return_tensors='pt').to(model.device)

# Generate text
generated_ids = model.generate(**tokens)

# Decode the output
generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

# Print the result
print(generated_text)



from auto_gptq import AutoGPTQForCausalLM
from transformers import AutoTokenizer

# Load the model
model = AutoGPTQForCausalLM.from_quantized("acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit", device="cuda:0", use_safetensors=True)

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path)

# Define the prompt
prompt = "What is the capital of France?"

# Tokenize the prompt
tokens = tokenizer(prompt, return_tensors='pt').to(model.device)

# Generate text with parameters
generated_ids = model.generate(**tokens,
                                 max_new_tokens=22,  # Adjust as needed
                                 temperature=0.2,   # Adjust as needed
                                 top_k=33,         # Adjust as needed
                                 top_p=0.5,        # Adjust as needed
                                 do_sample=True)   # Enable sampling

# Decode the output
generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

# Print the result
print(generated_text)

!CUDA_VISIBLE_DEVICES=0 python generation_speed.py --model_name_or_path PATH/TO/MODEL/DIR

!git clone https://github.com/AutoGPTQ/AutoGPTQ.git

# Commented out IPython magic to ensure Python compatibility.
# %cd AutoGPTQ
!CUDA_VISIBLE_DEVICES=0 python /content/AutoGPTQ/examples/benchmark/generation_speed.py --model_name_or_path acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit

import torch
from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# --- الإعدادات الأساسية ---
# اسم المجلد الذي حفظت فيه النموذج المضغوط محليًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"
# أو اسم النموذج على Hugging Face Hub إذا قمت برفعه
# repo_id = "YourUserName/opt-125m-4bit"

# اسم المجلد أو المستودع الخاص بالنموذج الأصلي (لتحميل الـ Tokenizer)
# (عادة ما يكون هو نفسه الذي استخدمته للضغط)
pretrained_model_dir = "meta-llama/Meta-Llama-3-8B-Instruct"

# --- تحميل الـ Tokenizer ---
# تحتاج إلى Tokenizer لتحويل النص إلى مدخلات للنموذج والعكس
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)

# --- تحميل النموذج المضغوط ---
# اختر طريقة واحدة للتحميل:

# 1. التحميل من مجلد محلي:
logging.info(f"Loading quantized model from local directory: {quantized_model_dir}")
model = AutoGPTQForCausalLM.from_quantized(
    quantized_model_dir,
    device="cuda:0",  # تحديد الجهاز (e.g., "cuda:0" for first GPU, "cpu")
    use_safetensors=True # يفضل استخدامه إذا حفظت بهذا التنسيق
)
logging.info("Quantized model loaded successfully.")

# 2. (بديل) التحميل من Hugging Face Hub:
# قم بإزالة التعليق إذا كنت تريد التحميل من Hub واستبدل repo_id
# logging.info(f"Loading quantized model from Hub: {repo_id}")
# model = AutoGPTQForCausalLM.from_quantized(
#     repo_id,
#     device="cuda:0",
#     use_safetensors=True,
#     # use_triton=False # قد تحتاج لتعديل هذا بناءً على بيئتك وإمكانيات GPU
# )
# logging.info("Quantized model loaded successfully from Hub.")


# --- طرق الاستدلال (Inference) ---

# الطريقة 1: استخدام model.generate
logging.info("Performing inference using model.generate...")
prompt = "auto-gptq is"
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
output = model.generate(inputs=input_ids, max_new_tokens=50) # يمكنك تعديل max_new_tokens
generated_text_generate = tokenizer.decode(output[0])
print("--- Output from model.generate ---")
print(generated_text_generate)
print("-" * 30)


# الطريقة 2: استخدام TextGenerationPipeline (أسهل للاستخدام)
logging.info("Performing inference using TextGenerationPipeline...")
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
prompt_pipeline = "auto-gptq is"
result_pipeline = pipeline(prompt_pipeline, max_length=60) # يمكنك تعديل max_length
generated_text_pipeline = result_pipeline[0]["generated_text"]
print("--- Output from Pipeline ---")
print(generated_text_pipeline)
print("-" * 30)

!huggingface-cli login --token XX

# مثال مع model.generate
output = model.generate(
    inputs=input_ids,
    max_new_tokens=50,
    temperature=0.7,
    top_k=50,
    top_p=0.95,
    repetition_penalty=1.15
 )
print(tokenizer.decode(output[0]))

# مثال مع pipeline
result_pipeline = pipeline(
    prompt_pipeline,
    max_new_tokens=50, # أو استخدم max_length
    temperature=0.7,
    top_k=50,
    top_p=0.95,
    repetition_penalty=1.15
)
print(result_pipeline[0]["generated_text"])

import torch
from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# --- الإعدادات الأساسية ---
# اسم المجلد الذي حفظت فيه النموذج المضغوط محليًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"
# أو اسم النموذج على Hugging Face Hub إذا قمت برفعه
# repo_id = "YourUserName/opt-125m-4bit"

# اسم المجلد أو المستودع الخاص بالنموذج الأصلي (لتحميل الـ Tokenizer)
# (عادة ما يكون هو نفسه الذي استخدمته للضغط)
pretrained_model_dir = "meta-llama/Meta-Llama-3-8B-Instruct"

# --- تحميل الـ Tokenizer ---
# تحتاج إلى Tokenizer لتحويل النص إلى مدخلات للنموذج والعكس
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)

# --- تحميل النموذج المضغوط ---
# اختر طريقة واحدة للتحميل:

# 1. التحميل من مجلد محلي:
logging.info(f"Loading quantized model from local directory: {quantized_model_dir}")
model = AutoGPTQForCausalLM.from_quantized(
    quantized_model_dir,
    device="cuda:0",  # تحديد الجهاز (e.g., "cuda:0" for first GPU, "cpu")
    use_safetensors=True # يفضل استخدامه إذا حفظت بهذا التنسيق
)
logging.info("Quantized model loaded successfully.")

# 2. (بديل) التحميل من Hugging Face Hub:
# قم بإزالة التعليق إذا كنت تريد التحميل من Hub واستبدل repo_id
# logging.info(f"Loading quantized model from Hub: {repo_id}")
# model = AutoGPTQForCausalLM.from_quantized(
#     repo_id,
#     device="cuda:0",
#     use_safetensors=True,
#     # use_triton=False # قد تحتاج لتعديل هذا بناءً على بيئتك وإمكانيات GPU
# )
# logging.info("Quantized model loaded successfully from Hub.")


# --- طرق الاستدلال (Inference) ---

# الطريقة 1: استخدام model.generate
logging.info("Performing inference using model.generate...")
prompt = "auto-gptq is"
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
output = model.generate(inputs=input_ids, max_new_tokens=50) # يمكنك تعديل max_new_tokens
generated_text_generate = tokenizer.decode(output[0])
print("--- Output from model.generate ---")
print(generated_text_generate)
print("-" * 30)


# الطريقة 2: استخدام TextGenerationPipeline (أسهل للاستخدام)
logging.info("Performing inference using TextGenerationPipeline...")
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
prompt_pipeline = "auto-gptq is"
result_pipeline = pipeline(prompt_pipeline, max_length=60) # يمكنك تعديل max_length
generated_text_pipeline = result_pipeline[0]["generated_text"]
print("--- Output from Pipeline ---")
print(generated_text_pipeline)
print("-" * 30)

import torch
from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# --- الإعدادات الأساسية ---
# اسم المجلد الذي حفظت فيه النموذج المضغوط محليًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"
# أو اسم النموذج على Hugging Face Hub إذا قمت برفعه
# repo_id = "YourUserName/opt-125m-4bit"

# اسم المجلد أو المستودع الخاص بالنموذج الأصلي (لتحميل الـ Tokenizer)
# (عادة ما يكون هو نفسه الذي استخدمته للضغط)
pretrained_model_dir = "meta-llama/Meta-Llama-3-8B-Instruct"

# --- تحميل الـ Tokenizer ---
# تحتاج إلى Tokenizer لتحويل النص إلى مدخلات للنموذج والعكس
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)

# --- تحميل النموذج المضغوط ---
# اختر طريقة واحدة للتحميل:

# 1. التحميل من مجلد محلي:
logging.info(f"Loading quantized model from local directory: {quantized_model_dir}")
model = AutoGPTQForCausalLM.from_quantized(
    quantized_model_dir,
    device="cuda:0",  # تحديد الجهاز (e.g., "cuda:0" for first GPU, "cpu")
    use_safetensors=True # يفضل استخدامه إذا حفظت بهذا التنسيق
)
logging.info("Quantized model loaded successfully.")


logging.info("Performing inference using model.generate...")
prompt = "auto-gptq is"
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
output = model.generate(inputs=input_ids, max_new_tokens=50) # يمكنك تعديل max_new_tokens
generated_text_generate = tokenizer.decode(output[0])
print("--- Output from model.generate ---")
print(generated_text_generate)
print("-" * 30)

output = model.generate(
    inputs=input_ids,
    max_new_tokens=50,
    temperature=0.7,
    top_k=50,
    top_p=0.95,
    repetition_penalty=1.15
 )
print(tokenizer.decode(output[0]))



import torch
from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# --- الإعدادات الأساسية ---
# اسم المجلد الذي حفظت فيه النموذج المضغوط محليًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"
# أو اسم النموذج على Hugging Face Hub إذا قمت برفعه
# repo_id = "YourUserName/opt-125m-4bit"

# اسم المجلد أو المستودع الخاص بالنموذج الأصلي (لتحميل الـ Tokenizer)
# (عادة ما يكون هو نفسه الذي استخدمته للضغط)
pretrained_model_dir = "meta-llama/Meta-Llama-3-8B-Instruct"

# --- تحميل الـ Tokenizer ---
# تحتاج إلى Tokenizer لتحويل النص إلى مدخلات للنموذج والعكس
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)

# --- تحميل النموذج المضغوط ---
# اختر طريقة واحدة للتحميل:

# 1. التحميل من مجلد محلي:
logging.info(f"Loading quantized model from local directory: {quantized_model_dir}")
model = AutoGPTQForCausalLM.from_quantized(
    quantized_model_dir,
    device="cuda:0",  # تحديد الجهاز (e.g., "cuda:0" for first GPU, "cpu")
    use_safetensors=True # يفضل استخدامه إذا حفظت بهذا التنسيق
)
logging.info("Quantized model loaded successfully.")


logging.info("Performing inference using model.generate...")
prompt = "python is "
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
output = model.generate(
    inputs=input_ids,
    max_new_tokens=50,
    temperature=0.2,
    top_k=20,
    top_p=0.40,
    do_sample=True,
    repetition_penalty=1.15
 )
print(tokenizer.decode(output[0]))
generated_text_generate = tokenizer.decode(output[0])
print("--- Output from model.generate ---")
print(generated_text_generate)
print("-" * 30)

import torch
from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# --- الإعدادات الأساسية ---
# اسم المجلد الذي حفظت فيه النموذج المضغوط محليًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"
# أو اسم النموذج على Hugging Face Hub إذا قمت برفعه
# repo_id = "YourUserName/opt-125m-4bit"

# اسم المجلد أو المستودع الخاص بالنموذج الأصلي (لتحميل الـ Tokenizer)
# (عادة ما يكون هو نفسه الذي استخدمته للضغط)
pretrained_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"

# --- تحميل الـ Tokenizer ---
# تحتاج إلى Tokenizer لتحويل النص إلى مدخلات للنموذج والعكس
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)

# --- تحميل النموذج المضغوط ---
# اختر طريقة واحدة للتحميل:

# 1. التحميل من مجلد محلي:
logging.info(f"Loading quantized model from local directory: {quantized_model_dir}")
model = AutoGPTQForCausalLM.from_quantized(
    quantized_model_dir,
    device="cuda:0",  # تحديد الجهاز (e.g., "cuda:0" for first GPU, "cpu")
    use_safetensors=True # يفضل استخدامه إذا حفظت بهذا التنسيق
)
logging.info("Quantized model loaded successfully.")


logging.info("Performing inference using model.generate...")
prompt = "python is "
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
output = model.generate(
    inputs=input_ids,
    max_new_tokens=50,
    temperature=0.2,
    top_k=20,
    top_p=0.40,
    do_sample=True,
    repetition_penalty=1.15
 )
print(tokenizer.decode(output[0]))
generated_text_generate = tokenizer.decode(output[0])
print("--- Output from model.generate ---")
print(generated_text_generate)
print("-" * 30)

import torch
from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# --- الإعدادات الأساسية ---
# اسم المجلد الذي حفظت فيه النموذج المضغوط محليًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"
# أو اسم النموذج على Hugging Face Hub إذا قمت برفعه
# repo_id = "YourUserName/opt-125m-4bit"

# اسم المجلد أو المستودع الخاص بالنموذج الأصلي (لتحميل الـ Tokenizer)
# (عادة ما يكون هو نفسه الذي استخدمته للضغط)
pretrained_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"

# --- تحميل الـ Tokenizer ---
# تحتاج إلى Tokenizer لتحويل النص إلى مدخلات للنموذج والعكس
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)

# --- تحميل النموذج المضغوط ---
# اختر طريقة واحدة للتحميل:

# 1. التحميل من مجلد محلي:
logging.info(f"Loading quantized model from local directory: {quantized_model_dir}")
model = AutoGPTQForCausalLM.from_quantized(
    quantized_model_dir,
    device="cuda:0",  # تحديد الجهاز (e.g., "cuda:0" for first GPU, "cpu")
    use_safetensors=True # يفضل استخدامه إذا حفظت بهذا التنسيق
)
logging.info("Quantized model loaded successfully.")


logging.info("Performing inference using model.generate...")
prompt = "ai is "
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
output = model.generate(
    inputs=input_ids,
    max_new_tokens=50,
    temperature=0.1,
    top_k=10,
    top_p=0.20,
    do_sample=True,
    repetition_penalty=1
 )
print(tokenizer.decode(output[0]))
generated_text_generate = tokenizer.decode(output[0])
print("--- Output from model.generate ---")
print(generated_text_generate)
print("-" * 30)

import torch
from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# --- الإعدادات الأساسية ---
# اسم المجلد الذي حفظت فيه النموذج المضغوط محليًا
quantized_model_dir = "acl-srw-2024/openthai-7b-unsloth-gptq-2bit"
# أو اسم النموذج على Hugging Face Hub إذا قمت برفعه
# repo_id = "YourUserName/opt-125m-4bit"

# اسم المجلد أو المستودع الخاص بالنموذج الأصلي (لتحميل الـ Tokenizer)
# (عادة ما يكون هو نفسه الذي استخدمته للضغط)
pretrained_model_dir = "acl-srw-2024/openthai-7b-unsloth-gptq-2bit"

# --- تحميل الـ Tokenizer ---
# تحتاج إلى Tokenizer لتحويل النص إلى مدخلات للنموذج والعكس
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)

# --- تحميل النموذج المضغوط ---
# اختر طريقة واحدة للتحميل:

# 1. التحميل من مجلد محلي:
logging.info(f"Loading quantized model from local directory: {quantized_model_dir}")
model = AutoGPTQForCausalLM.from_quantized(
    quantized_model_dir,
    device="cuda:0",  # تحديد الجهاز (e.g., "cuda:0" for first GPU, "cpu")
    use_safetensors=True # يفضل استخدامه إذا حفظت بهذا التنسيق
)
logging.info("Quantized model loaded successfully.")


logging.info("Performing inference using model.generate...")
prompt = "python is "
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
output = model.generate(
    inputs=input_ids,
    max_new_tokens=50,
    temperature=0.2,
    top_k=20,
    top_p=0.40,
    do_sample=True,
    repetition_penalty=1.15
 )
print(tokenizer.decode(output[0]))
generated_text_generate = tokenizer.decode(output[0])
print("--- Output from model.generate ---")
print(generated_text_generate)
print("-" * 30)
print(tokenizer.decode(model.generate(**tokenizer("auto_gptq is", return_tensors="pt").to(model.device))[0]))

import torch
from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# --- الإعدادات الأساسية ---
# اسم المجلد الذي حفظت فيه النموذج المضغوط محليًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"
# أو اسم النموذج على Hugging Face Hub إذا قمت برفعه
# repo_id = "YourUserName/opt-125m-4bit"

# اسم المجلد أو المستودع الخاص بالنموذج الأصلي (لتحميل الـ Tokenizer)
# (عادة ما يكون هو نفسه الذي استخدمته للضغط)
pretrained_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"

# --- تحميل الـ Tokenizer ---
# تحتاج إلى Tokenizer لتحويل النص إلى مدخلات للنموذج والعكس
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)

# --- تحميل النموذج المضغوط ---
# اختر طريقة واحدة للتحميل:

# 1. التحميل من مجلد محلي:
logging.info(f"Loading quantized model from local directory: {quantized_model_dir}")
model = AutoGPTQForCausalLM.from_quantized(
    quantized_model_dir,
    device="cuda:0",  # تحديد الجهاز (e.g., "cuda:0" for first GPU, "cpu")
    use_safetensors=True # يفضل استخدامه إذا حفظت بهذا التنسيق
)
logging.info("Quantized model loaded successfully.")


logging.info("Performing inference using model.generate...")
prompt = "python is "
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
output = model.generate(
    inputs=input_ids,
    max_new_tokens=50,
    temperature=0.2,
    top_k=20,
    top_p=0.40,
    do_sample=True,
    repetition_penalty=1.15
 )
print(tokenizer.decode(output[0]))
generated_text_generate = tokenizer.decode(output[0])
print("--- Output from model.generate ---")
print(generated_text_generate)
print("-" * 30)
print(tokenizer.decode(model.generate(**tokenizer("auto_gptq is", return_tensors="pt").to(model.device))[0]))

from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

pretrained_model_dir = "meta-llama/Meta-Llama-3-8B-Instruct"
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"

tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)
examples = [
    tokenizer(
        "auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm."
    )
]

quantize_config = BaseQuantizeConfig(
    bits=4,  # quantize model to 4-bit
    group_size=128,  # it is recommended to set the value to 128
    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad
)

# load un-quantized model, by default, the model will always be loaded into CPU memory
model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)

# quantize model, the examples should be list of dict whose keys can only be "input_ids" and "attention_mask"
model.quantize(examples)

# save quantized model
model.save_quantized(quantized_model_dir)

# save quantized model using safetensors
model.save_quantized(quantized_model_dir, use_safetensors=True)

# push quantized model to Hugging Face Hub.
# to use use_auth_token=True, Login first via huggingface-cli login.
# or pass explcit token with: use_auth_token="hf_xxxxxxx"
# (uncomment the following three lines to enable this feature)
# repo_id = f"YourUserName/{quantized_model_dir}"
# commit_message = f"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}"
# model.push_to_hub(repo_id, commit_message=commit_message, use_auth_token=True)

# alternatively you can save and push at the same time
# (uncomment the following three lines to enable this feature)
# repo_id = f"YourUserName/{quantized_model_dir}"
# commit_message = f"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}"
# model.push_to_hub(repo_id, save_dir=quantized_model_dir, use_safetensors=True, commit_message=commit_message, use_auth_token=True)

# load quantized model to the first GPU
model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device="cuda:0")

# download quantized model from Hugging Face Hub and load to the first GPU
# model = AutoGPTQForCausalLM.from_quantized(repo_id, device="cuda:0", use_safetensors=True, use_triton=False)

# inference with model.generate
print(tokenizer.decode(model.generate(**tokenizer("auto_gptq is", return_tensors="pt").to(model.device))[0]))

# or you can also use pipeline
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
print(pipeline("auto-gptq is")[0]["generated_text"])

from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# مجلد النموذج المكمم مسبقًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"

# تحميل التوكنيزيشن من النموذج الأصلي
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct", use_fast=True)

# تحميل النموذج المكمم مباشرة على الـ GPU
model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device="cuda:0")

# اختبار التوليد باستخدام model.generate
print(tokenizer.decode(model.generate(**tokenizer("auto_gptq is", return_tensors="pt").to(model.device))[0]))

# أو استخدام pipeline
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
print(pipeline("auto-gptq is")[0]["generated_text"])

acl-srw-2024/openthai-7b-unsloth-gptq-2bit



from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# مجلد النموذج المكمم مسبقًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-3bit"

# تحميل التوكنيزيشن من النموذج الأصلي
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct", use_fast=True)

# تحميل النموذج المكمم مباشرة على الـ GPU
model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device="cuda:0")

# اختبار التوليد باستخدام model.generate
print(tokenizer.decode(model.generate(**tokenizer("auto_gptq is", return_tensors="pt").to(model.device))[0]))

# أو استخدام pipeline
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
print(pipeline("auto-gptq is")[0]["generated_text"])

from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# مجلد النموذج المكمم مسبقًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"

# تحميل التوكنيزيشن من النموذج الأصلي
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct", use_fast=True)

# تحميل النموذج المكمم مباشرة على الـ GPU
model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device="cuda:0")

# توليد النص باستخدام model.generate مع المعاملات المطلوبة
inputs = tokenizer("auto_gptq is", return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    do_sample=True,
    temperature=0.2,
    top_p=0.4,
    max_new_tokens=100
)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# أو استخدام pipeline مع نفس الإعدادات
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
print(pipeline(
    "auto-gptq is",
    do_sample=True,
    temperature=0.2,
    top_p=0.4,
    max_new_tokens=100
)[0]["generated_text"])

from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# مجلد النموذج المكمم مسبقًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-3bit"

# تحميل التوكنيزيشن من النموذج الأصلي
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct", use_fast=True)

# تحميل النموذج المكمم مباشرة على الـ GPU
model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device="cuda:0")

# توليد النص باستخدام model.generate مع المعاملات المطلوبة
inputs = tokenizer("auto_gptq is", return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    do_sample=True,
    temperature=0.2,
    top_p=0.4,
    max_new_tokens=100
)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# أو استخدام pipeline مع نفس الإعدادات
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
print(pipeline(
    "auto-gptq is",
    do_sample=True,
    temperature=0.2,
    top_p=0.4,
    max_new_tokens=100
)[0]["generated_text"])

from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# مجلد النموذج المكمم مسبقًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-3bit"

# تحميل التوكنيزيشن من النموذج الأصلي
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct", use_fast=True)

# تحميل النموذج المكمم مباشرة على الـ GPU
model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device="cuda:0")

# اختبار التوليد باستخدام model.generate
print(tokenizer.decode(model.generate(**tokenizer("what is python?", return_tensors="pt").to(model.device))[0]))

# أو استخدام pipeline
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
print(pipeline("what is python?")[0]["generated_text"])









!python /content/AutoGPTQ/examples/benchmark/generation_speed.py --model_name_or_path acl-srw-2024/llama-3-8b-instruct-scb-gptq-3bit



from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# مجلد النموذج المكمم مسبقًا
quantized_model_dir = "acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2"

# تحميل التوكنيزيشن من النموذج الأصلي
tokenizer = AutoTokenizer.from_pretrained("unsloth/Meta-Llama-3.1-8B-bnb-4bit", use_fast=True)

# تحميل النموذج المكمم مباشرة على الـ GPU
model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device="cuda:0")

# توليد النص باستخدام model.generate مع المعاملات المطلوبة
inputs = tokenizer("auto_gptq is", return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    do_sample=True,
    temperature=0.2,
    top_p=0.4,
    max_new_tokens=100
)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# أو استخدام pipeline مع نفس الإعدادات
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
print(pipeline(
    "auto-gptq is",
    do_sample=True,
    temperature=0.2,
    top_p=0.4,
    max_new_tokens=100
)[0]["generated_text"])

from transformers import AutoTokenizer, AutoModelForCausalLM, TextGenerationPipeline
import torch

model_id = "acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2"

tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.float16,
    load_in_4bit=True  # <-- هنا
)

# توليد النص
inputs = tokenizer("auto_gptq is", return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    do_sample=True,
    temperature=0.2,
    top_p=0.4,
    max_new_tokens=100
)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# مجلد النموذج المكمم
quantized_model_dir = "acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2"

# استخدم توكنيزيشن النموذج الأصلي المبني عليه (مثلاً نسخة LLaMA 3.1 من UnsLoTH)
tokenizer = AutoTokenizer.from_pretrained("unsloth/Meta-Llama-3.1-8B", use_fast=True)

# تحميل النموذج المكمم من مجلد GPTQ
model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device="cuda:0")

# توليد النص
inputs = tokenizer("auto_gptq is", return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    do_sample=True,
    temperature=0.2,
    top_p=0.4,
    max_new_tokens=100
)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# استخدام pipeline بنفس الإعدادات
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
print(pipeline(
    "auto-gptq is",
    do_sample=True,
    temperature=0.2,
    top_p=0.4,
    max_new_tokens=100
)[0]["generated_text"])

from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2"
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16,
    load_in_4bit=True
)

from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s",
    level=logging.INFO,
    datefmt="%Y-%m-%d %H:%M:%S"
)

# مجلد النموذج المكمم مسبقًا
quantized_model_dir = "acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2"

# تحميل التوكنيزيشن من نموذج LLama3 الأصلي (مطلوب لتجنب الخطأ)
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct", use_fast=True)

# تحميل النموذج المكمم مباشرة على الـ GPU
model = AutoGPTQForCausalLM.from_quantized(
    quantized_model_dir,
    device="cuda:0",
    use_safetensors=True,
    trust_remote_code=True
)

# توليد نصوص
inputs = tokenizer("auto_gptq is", return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    do_sample=True,
    temperature=0.2,
    top_p=0.4,
    max_new_tokens=100
)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# عبر pipeline
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
print(pipeline(
    "auto-gptq is",
    do_sample=True,
    temperature=0.2,
    top_p=0.4,
    max_new_tokens=100
)[0]["generated_text"])

# Load model script
from auto_gptq import AutoGPTQForCausalLM
model = AutoGPTQForCausalLM.from_quantized("acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2", device="cuda:0",use_safetensors=True)



# Load model script
from auto_gptq import AutoGPTQForCausalLM
model = AutoGPTQForCausalLM.from_quantized("acl-srw-2024/llama3-8b-unsloth-sft-awq-8bit-v4", device="cuda:0",use_safetensors=True)



# Load model script
from auto_gptq import AutoGPTQForCausalLM
model = AutoGPTQForCausalLM.from_quantized("acl-srw-2024/llama3-8b-unsloth-sft-quip-2bit", device="cuda:0",use_safetensors=True)

# Load model script
from auto_gptq import AutoGPTQForCausalLM
# Either try acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2 which is quantized with GPTQ
# or acl-srw-2024/llama3-8b-unsloth-sft-awq-8bit-v4 which is quantized with AWQ.
model = AutoGPTQForCausalLM.from_quantized("acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2", device="cuda:0",use_safetensors=True)

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("acl-srw-2024/llama3-8b-unsloth-sft-awq-8bit-v4")
model = AutoModelForCausalLM.from_pretrained("acl-srw-2024/llama3-8b-unsloth-sft-awq-8bit-v4")

!pip install autoawq

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("acl-srw-2024/llama3-8b-unsloth-sft-awq-8bit-v4")
model = AutoModelForCausalLM.from_pretrained("acl-srw-2024/llama3-8b-unsloth-sft-awq-8bit-v4")

#!pip install autoawq # Make sure autoawq is installed

from transformers import AutoTokenizer, AutoModelForCausalLM

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("acl-srw-2024/llama3-8b-unsloth-sft-awq-8bit-v4")

# Load the model with AutoAWQForCausalLM instead of AutoModelForCausalLM
from autoawq import AutoAWQForCausalLM
model = AutoAWQForCausalLM.from_pretrained("acl-srw-2024/llama3-8b-unsloth-sft-awq-8bit-v4", device="cuda:0", trust_remote_code=True)

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# تحميل المعمارية الافتراضية لـ LLaMA 3
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3-8B")

# تحميل state_dict
state_dict = torch.load("model.pt", map_location="cpu")
model.load_state_dict(state_dict)

model.eval()

acl-srw-2024/mistral-7b-unsloth-sft-quip-2bit

!pip install auto-gptq

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
model_name = "acl-srw-2024/mistral-7b-unsloth-sft-quip-2bit"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
model_name = "acl-srw-2024/mistral-7b-unsloth-sft-quip-2bit"
tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    offload_folder="/content/offload",
    torch_dtype="auto"
)

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
model_name = "acl-srw-2024/mistral-7b-unsloth-sft-quip-2bit"
tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="cuda",
    offload_folder="offload",  # تحديد مجلد للتفريغ
    torch_dtype="auto"
)

input_text = "what is ai?"
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=10)

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)















"""### شغال"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
model_name = "acl-srw-2024/mistral-7b-unsloth-sft-quip-2bit"
tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="cuda",
    offload_folder="offload",  # تحديد مجلد للتفريغ
    torch_dtype="auto"
)

input_text = "what is ai?"
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=128)

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)



!pip install autoawq

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("acl-srw-2024/llama3-8b-unsloth-sft-awq-4bit-v4")
model = AutoModelForCausalLM.from_pretrained("acl-srw-2024/llama3-8b-unsloth-sft-awq-4bit-v4")

input_text = "what is ai?"
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=128)

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)



from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("acl-srw-2024/llama3-8b-unsloth-sft-awq-4bit-v4")
model = AutoModelForCausalLM.from_pretrained("acl-srw-2024/llama3-8b-unsloth-sft-awq-4bit-v4")

# Define your input text
input_text = "ما هو الذكاء الاصطناعي؟"  # Example input in Arabic

# Tokenize the input
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

# Generate text
with torch.no_grad():  # Disable gradient calculation for inference
    outputs = model.generate(**inputs, max_new_tokens=128)  # Adjust max_new_tokens as needed

# Decode the output
response = tokenizer.decode(outputs[0], skip_special_tokens=True)

# Print the response
print(response)

"""aشغال"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
# Set device to Vega
device = torch.device('cuda') # Assuming 'vega' is the device name

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("acl-srw-2024/llama3-8b-unsloth-sft-awq-4bit-v4")
model = AutoModelForCausalLM.from_pretrained("acl-srw-2024/llama3-8b-unsloth-sft-awq-4bit-v4").to(device) # Specify device during loading


# Define your input text
input_text = "what is python"  # Example input in Arabic

# Tokenize the input
inputs = tokenizer(input_text, return_tensors="pt").to(device)  # Move input to device

# Generate text
with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=10)

# Decode and print the response
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
# Set device to Vega
device = torch.device('cuda') # Assuming 'vega' is the device name

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("acl-srw-2024/llama3-8b-unsloth-sft-awq-4bit-v4")
model = AutoModelForCausalLM.from_pretrained("acl-srw-2024/llama3-8b-unsloth-sft-awq-4bit-v4").to(device) # Specify device during loading


# Define your input text
input_text = "what is python؟"  # Example input in Arabic

# Tokenize the input
inputs = tokenizer(input_text, return_tensors="pt").to(device)  # Move input to device

# Generate text
with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=128)

# Decode and print the response
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)







import torch

# تحميل النموذج
model = torch.load("acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2")
model.eval()  # وضع النموذج في وضع الاستدلال (inference mode)

# بيانات تجريبية (مثلاً بيانات عشوائية بحجم مناسب)
# عدل حجم البيانات لتناسب النموذج الذي عندك
dummy_input = torch.randn(1, 3, 224, 224)  # مثال لصورة بحجم 224x224

# استدلال
with torch.no_grad():
    output = model(dummy_input)

print("Output:", output)

!pip install pretrainedmodels

import pretrainedmodels

print(pretrainedmodels["acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2"])

import pretrainedmodels

# Get a list of available model names
model_names = pretrainedmodels.model_names

# Check if your desired model is in the list
if "acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2" in model_names:
    print("Model found!")
else:
    print("Model not found in pretrainedmodels. Please check the available model names.")

# To load a specific model, use the pretrainedmodels.pretrained function
# For example (Replace with the name of a valid pretrainedmodels model

model_name = 'acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2' # could be fbresnet152 or inceptionresnetv2
model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')
model.eval()

import torch
import pretrainedmodels.utils as utils

load_img = utils.LoadImage()
model = torch.load("acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2")
# transformations depending on the model
# rescale, center crop, normalize, and others (ex: ToBGR, ToRange255)
tf_img = utils.TransformImage(model)

path_img = 'data/cat.jpg'

input_img = load_img(path_img)
input_tensor = tf_img(input_img)         # 3x400x225 -> 3x299x299 size may differ
input_tensor = input_tensor.unsqueeze(0) # 3x299x299 -> 1x3x299x299
input = torch.autograd.Variable(input_tensor,
    requires_grad=False)

output_logits = model(input) # 1x1000

model = timm.create_model('acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2', pretrained=True, img_size=512, features_only=True, out_indices=(-3, -2,))
output = model(torch.randn(2, 3, 512, 512))

for o in output:
    print(o.shape)
torch.Size([2, 768, 32, 32])
torch.Size([2, 768, 32, 32])









!huggingface-cli login --token XXXXX

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# تحميل المعمارية الافتراضية لـ LLaMA 3
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3-8B")

# تحميل state_dict
state_dict = torch.load("acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2", map_location="cuda")
model.load_state_dict(state_dict)

model.eval()

!huggingface-cli login

from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2"
tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)
model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=True)

from unsloth import FastLanguageModel
import torch

# تحديد مسار النموذج
model_path = "acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2"

# تحميل النموذج باستخدام Unsloth
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_path,
)

# تهيئة النموذج للاستدلال
FastLanguageModel.for_inference(model)

import torch
from unsloth import FastLanguageModel
from transformers import TextStreamer

# معرف الموديل من Hugging Face
model_id = "acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2"

# حدد الحد الأقصى لطول التسلسل (تأكد من أنه مناسب للموديل)
max_seq_length = 2048 # أو القيمة المستخدمة أثناء تدريب الموديل

# تحميل الموديل باستخدام Unsloth
# Unsloth سيكتشف تلقائياً أن هذا موديل QuIP# ولا يتطلب load_in_4bit=True
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_id,
    max_seq_length = max_seq_length,
    dtype = None,        # دع Unsloth يكتشف النوع المناسب (Let Unsloth autodetect)
    load_in_4bit = False, # مهم جداً: يجب أن يكون False لتحميل موديلات QuIP#
)

# لا حاجة لتطبيق PEFT إذا كنت تقوم بالاستدلال فقط على الموديل المحمل
# model = FastLanguageModel.get_peft_model(...) # هذا ليس ضرورياً هنا

print("Model and tokenizer loaded successfully!")

# --- مثال على الاستخدام (Inference Example) ---

# استخدم جهاز GPU (تأكد من تغيير "cuda:0" إذا كان لديك أكثر من GPU)
device = "cuda:0"
model.to(device)
model.eval() # ضعه في وضع التقييم

# أداة لعرض النص أثناء توليده (اختياري)
text_streamer = TextStreamer(tokenizer)

# النص الذي تريد أن يبدأ به الموديل
prompt = "ما هي فوائد الذكاء الاصطناعي في التعليم؟" # مثال: "What are the benefits of AI in education?"
inputs = tokenizer([prompt], return_tensors="pt").to(device)

print("\nGenerating response...")
# توليد النص
outputs = model.generate(
    **inputs,
    streamer=text_streamer,
    max_new_tokens=150,      # حدد عدد التوكنات الجديدة التي تريد توليدها
    do_sample=True,         # استخدم العينات لنتائج أكثر إبداعاً
    temperature=0.7,        # درجة حرارة العينة
    top_p=0.9,              # Top-p sampling
    # يمكنك تجربة إعدادات أخرى
)

# إذا لم تستخدم streamer، يمكنك فك تشفير المخرجات هكذا:
# decoded_output = tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]
# print("\n--- Generated Text ---")
# print(decoded_output)
# print("----------------------")

print("\nGeneration complete.")

# تثبيت أحدث إصدار من Unsloth من GitHub (يوصى به عادةً للحصول على أحدث الميزات وإصلاحات الأخطاء)
# سيقوم هذا الأمر أيضاً بتثبيت PyTorch, Transformers, TRL, إلخ.
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"

# قد تحتاج أيضاً لتثبيت هذه الاعتماديات الإضافية بشكل منفصل، خاصةً للكميات المنخفضة جداً مثل 2-bit
pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes

!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"

import torch
from unsloth import FastLanguageModel
from transformers import TextStreamer

# معرف الموديل من Hugging Face
model_id = "acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2"

# حدد الحد الأقصى لطول التسلسل (تأكد من أنه مناسب للموديل)
max_seq_length = 2048 # أو القيمة المستخدمة أثناء تدريب الموديل

# تحميل الموديل باستخدام Unsloth
# Unsloth سيكتشف تلقائياً أن هذا موديل QuIP# ولا يتطلب load_in_4bit=True
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_id,
    max_seq_length = max_seq_length,
    dtype = None,        # دع Unsloth يكتشف النوع المناسب (Let Unsloth autodetect)
    load_in_4bit = False, # مهم جداً: يجب أن يكون False لتحميل موديلات QuIP#
)

# لا حاجة لتطبيق PEFT إذا كنت تقوم بالاستدلال فقط على الموديل المحمل
# model = FastLanguageModel.get_peft_model(...) # هذا ليس ضرورياً هنا

print("Model and tokenizer loaded successfully!")

# --- مثال على الاستخدام (Inference Example) ---

# استخدم جهاز GPU (تأكد من تغيير "cuda:0" إذا كان لديك أكثر من GPU)
device = "cuda:0"
model.to(device)
model.eval() # ضعه في وضع التقييم

# أداة لعرض النص أثناء توليده (اختياري)
text_streamer = TextStreamer(tokenizer)

# النص الذي تريد أن يبدأ به الموديل
prompt = "hi" # مثال: "What are the benefits of AI in education?"
inputs = tokenizer([prompt], return_tensors="pt").to(device)

print("\nGenerating response...")
# توليد النص
outputs = model.generate(
    **inputs,
    streamer=text_streamer,
    max_new_tokens=150,      # حدد عدد التوكنات الجديدة التي تريد توليدها
    do_sample=True,         # استخدم العينات لنتائج أكثر إبداعاً
    temperature=0.7,        # درجة حرارة العينة
    top_p=0.9,              # Top-p sampling
    # يمكنك تجربة إعدادات أخرى
)

# إذا لم تستخدم streamer، يمكنك فك تشفير المخرجات هكذا:
# decoded_output = tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]
# print("\n--- Generated Text ---")
# print(decoded_output)
# print("----------------------")

print("\nGeneration complete.")

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchsummary import summary

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # PyTorch v0.4.0
model = Net().to(device)

summary(model, (1, 28, 28))

import torch
from torchvision import models
from torchsummary import summary

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
vgg = models.vgg16().to(device)

summary(vgg, (3, 224, 224))

import torch
import torch.nn as nn
from torchsummary import summary

class SimpleConv(nn.Module):
    def __init__(self):
        super(SimpleConv, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
        )

    def forward(self, x, y):
        x1 = self.features(x)
        x2 = self.features(y)
        return x1, x2

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = SimpleConv().to(device)

summary(model, [(1, 16, 16), (1, 28, 28)])

from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2",
    max_seq_length = 2048, # أو القيمة المناسبة
    dtype = None,          # مهم جداً لـ QuIP#
    load_in_4bit = False,  # ****** يجب أن تكون False لتحميل موديلات QuIP# ******
)

OSError: acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2 does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.

import torch

# تحميل النموذج
model = torch.load('model.pt', map_location=torch.device('cuda'))  # إذا كنت تستخدم GPU، استبدل بـ 'cuda'
model.eval()  # وضع النموذج في وضع الاستدلال

# تحضير بيانات الإدخال
# تأكد من أن الإدخال له نفس الأبعاد التي تم تدريب النموذج عليها
input_data = torch.randn(1, 3, 112, 112)  # مثال على بيانات إدخال (Batch size, Channels, Height, Width)

# إجراء الاستدلال
with torch.no_grad():  # تعطيل حساب التدرجات (Gradients)
    output = model(input_data)

# عرض النتائج
print(output)

import torch

# تحميل النموذج
model = torch.load('acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2', map_location=torch.device('cuda'))  # إذا كنت تستخدم GPU، استبدل بـ 'cuda'
model.eval()  # وضع النموذج في وضع الاستدلال

# تحضير بيانات الإدخال
# تأكد من أن الإدخال له نفس الأبعاد التي تم تدريب النموذج عليها
input_data = torch.randn(1, 3, 112, 112)  # مثال على بيانات إدخال (Batch size, Channels, Height, Width)

# إجراء الاستدلال
with torch.no_grad():  # تعطيل حساب التدرجات (Gradients)
    output = model(input_data)

# عرض النتائج
print(output)

!صلث

import torch

# تحميل النموذج من ملف محلي
model_path = 'path_to_downloaded_model/model.pt'  # قم بتعديل هذا المسار حسب موقع الملف
model = torch.load(model_path, map_location=torch.device('cpu'))  # استخدم 'cuda' إذا كنت تعمل على GPU
model.eval()  # وضع النموذج في وضع الاستدلال

print("Model loaded successfully!")

import torch
import requests

# تنزيل النموذج من المستودع
url = 'https://github.com/acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2/raw/main/model.pt'  # تأكد من أن الرابط صحيح
response = requests.get(url)
model_path = 'model.pt'

# حفظ النموذج محليًا
with open(model_path, 'wb') as f:
    f.write(response.content)

# تحميل النموذج
model = torch.load(model_path, map_location=torch.device('cpu'), weights_only=False)  # استخدم 'cuda' عند العمل على GPU
model.eval()  # وضع النموذج في وضع الاستدلال

print("Model downloaded and loaded successfully!")

with open(model_path, 'r', encoding='utf-8') as f:
    print(f.read())

import torch

model_path = 'model.pt'  # تأكد من أن المسار صحيح
model = torch.load(model_path, map_location=torch.device('cpu'), weights_only=False)
model.eval()

print("Model loaded successfully!")

"""ayhgشغال"""

import requests

# رابط النموذج
url = "https://huggingface.co/acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2/resolve/main/model.pt"
model_path = "model.pt"

# تحميل الملف وحفظه محليًا
response = requests.get(url, stream=True)
if response.status_code == 200:
    with open(model_path, "wb") as f:
        for chunk in response.iter_content(chunk_size=1024):
            if chunk:
                f.write(chunk)
    print("Model downloaded successfully and saved as model.pt!")
else:
    print(f"Failed to download the model. Status code: {response.status_code}")

import torch
!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
# 1. تحميل النموذج
model_path = "model.pt"  # مسار ملف النموذج
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # استخدام GPU إذا كان متاحًا
model = torch.load(model_path, map_location=device)
model.eval()  # وضع النموذج في وضع الاستدلال


torch.cuda.empty_cache()
# 2. إعداد بيانات الإدخال
# قم بتعديل هذا الجزء ليتوافق مع شكل بيانات الإدخال (Batch size, Channels, Height, Width)
# كمثال: إذا كان النموذج يتعامل مع صور 112x112 بثلاث قنوات (RGB)، يمكنك استخدام:
input_data = torch.randn(1, 3, 112, 112).to(device)  # بيانات عشوائية، استبدلها ببيانات حقيقية

# 3. تنفيذ الاستدلال
with torch.no_grad():  # تعطيل حساب التدرجات لتحسين الأداء
    output = model(input_data)

# 4. عرض النتيجة
print("Inference Output:", output)

device = torch.device("cpu")  # استخدام CPU
model = model.to(device)
input_data = input_data.to(device)

import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

import torch
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
# 1. تحميل النموذج
model_path = "model.pt"  # مسار ملف النموذج

#model_path = "model.pt"  # مسار ملف النموذج
#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # استخدام GPU إذا كان متاحًا
device = torch.device("cuda")  # استخدام CPU

torch.cuda.empty_cache()
model = torch.load(model_path, map_location=device)
model.eval()  # وضع النموذج في وضع الاستدلال


torch.cuda.empty_cache()

input_data = torch.randn(1, 3, 112, 112).to(device)

with torch.no_grad():  # تعطيل حساب التدرجات لتحسين الأداء
    output = model(input_data)

print("Inference Output:", output)

import torch
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
# 1. تحميل النموذج
model_path = "model.pt"  # مسار ملف النموذج
#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # استخدام GPU إذا كان متاحًا
device = torch.device("cuda")  # استخدام CPU
model = model_path.(device)
input_data = input_data.to(device)
model = torch.load(model_path, map_location=device)
model.eval()  # وضع النموذج في وضع الاستدلال


torch.cuda.empty_cache()
# 2. إعداد بيانات الإدخال
# قم بتعديل هذا الجزء ليتوافق مع شكل بيانات الإدخال (Batch size, Channels, Height, Width)
# كمثال: إذا كان النموذج يتعامل مع صور 112x112 بثلاث قنوات (RGB)، يمكنك استخدام:
input_data = torch.randn(1, 3, 112, 112).to(device)  # بيانات عشوائية، استبدلها ببيانات حقيقية

# 3. تنفيذ الاستدلال
with torch.no_grad():  # تعطيل حساب التدرجات لتحسين الأداء
    output = model(input_data)

# 4. عرض النتيجة
print("Inference Output:", output)













from transformers import AutoModel, AutoTokenizer
import torch
# تحميل النموذج والمفكرة (tokenizer)
model_name = "/content/model.pt"  # استبدل باسم النموذج أو المسار
tokenizer = AutoTokenizer.from_pretrained(model_name)

# تحميل النموذج مع تعيين device_map
model = AutoModel.from_pretrained(
    model_name,
    device_map="auto"  # توزيع تلقائي للنموذج على الأجهزة المتاحة
)

# طباعة خريطة الأجهزة
print("Device map:", model.hf_device_map)

# اختبار الاستدلال
inputs = tokenizer("اختبار النص", return_tensors="pt")  # مثال على الإدخال
outputs = model(**inputs)

print("Inference completed successfully!")

import torch

# تحديد مسار النموذج
model_path = "مسار_الملف/model.pt"



# تهيئة النموذج للاستدلال
FastLanguageModel.for_inference(model)

from transformers import TextStreamer

# إعداد الإدخال
input_text = "ما هو الذكاء الاصطناعي؟"
inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

# تنفيذ الاستدلال
text_streamer = TextStreamer(tokenizer)
outputs = model.generate(**inputs, streamer=text_streamer, max_new_tokens=100)

# عرض النتيجة
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)





from transformers import AutoModel, AutoTokenizer

# 1. تحميل النموذج باستخدام transformers
model_name = "/content/model.pt"  # اسم أو مسار النموذج
device_map = "auto"  # توزيع تلقائي للنموذج على الأجهزة المتاحة (CPU/GPU)

# تحميل النموذج مع إعداد device_map
model = AutoModel.from_pretrained(
    model_name,
    device_map=device_map
)
model.eval()  # وضع النموذج في وضع الاستدلال

# 2. إعداد بيانات الإدخال
# قم بتعديل هذا الجزء ليتوافق مع شكل بيانات الإدخال
input_data = torch.randn(1, 3, 112, 112).to(model.device)  # بيانات عشوائية، استبدلها ببيانات حقيقية

# 3. تنفيذ الاستدلال
with torch.no_grad():  # تعطيل حساب التدرجات لتحسين الأداء
    output = model(input_data)

# 4. عرض النتيجة
print("Inference Output:", output)











"""شغ"""

import torch
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
# 1. تحميل النموذج
model_path = "model.pt"  # مسار ملف النموذج

#model_path = "model.pt"  # مسار ملف النموذج
#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # استخدام GPU إذا كان متاحًا
device = torch.device("cuda")  # استخدام CPU

torch.cuda.empty_cache()
model = torch.load(model_path, map_location=device)
model.eval()  # وضع النموذج في وضع الاستدلال


torch.cuda.empty_cache()
input_data = torch.randn(1, 3, 112, 112).to(device)
with torch.no_grad():  # تعطيل حساب التدرجات لتحسين الأداء
    output = model(input_data)
print("Inference Output:", output)

"""https://huggingface.co/acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2/blob/main/model.pt"""





!wget https://huggingface.co/rfs89y/model_2600000.pt/resolve/main/model_2600000.pt