# -*- coding: utf-8 -*-
"""Load llama gptq 2 bit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uqTupx2RNrm1jkzoPLWhU2HouVheDkV2

### https://huggingface.co/acl-srw-2024

https://github.com/AutoGPTQ/AutoGPTQ

https://github.com/Cornell-RelaxML/QuIP
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install auto-gptq

# Load model script
from auto_gptq import AutoGPTQForCausalLM
model = AutoGPTQForCausalLM.from_quantized("acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit", device="cuda:0",use_safetensors=True)







# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install auto-gptq

# Load model script
from auto_gptq import AutoGPTQForCausalLM
model = AutoGPTQForCausalLM.from_quantized("acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit", device="cuda:0",use_safetensors=True)

prompt = "What is the capital of France?"  # Replace with your desired prompt
tokens = model.tokenizer(prompt, return_tensors='pt').to(model.device)
generated_ids = model.generate(**tokens)
generated_text = model.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(generated_text)



from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path)
# ... (Rest of your existing code)
tokens = tokenizer(prompt, return_tensors='pt').to(model.device)
generated_ids = model.generate(**tokens)
generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
print(generated_text)

from auto_gptq import AutoGPTQForCausalLM
from transformers import AutoTokenizer

# Load the model
model = AutoGPTQForCausalLM.from_quantized("acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit", device="cuda:0",use_safetensors=True)

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path)

# Define the prompt
prompt = "who is ai?"  # Your prompt in Arabic

# Tokenize the prompt
tokens = tokenizer(prompt, return_tensors='pt').to(model.device)

# Generate text
generated_ids = model.generate(**tokens)

# Decode the output
generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

# Print the result
print(generated_text)

from auto_gptq import AutoGPTQForCausalLM
from transformers import AutoTokenizer

# Load the model
model = AutoGPTQForCausalLM.from_quantized("acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit", device="cuda:0",use_safetensors=True)

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path)

# Define the prompt
prompt = "who is ai?"  # Your prompt

# Tokenize the prompt
tokens = tokenizer(prompt, return_tensors='pt').to(model.device)

# Generate text
generated_ids = model.generate(**tokens)

# Decode the output
generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

# Print the result
print(generated_text)



from auto_gptq import AutoGPTQForCausalLM
from transformers import AutoTokenizer

# Load the model
model = AutoGPTQForCausalLM.from_quantized("acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit", device="cuda:0", use_safetensors=True)

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path)

# Define the prompt
prompt = "What is the capital of France?"

# Tokenize the prompt
tokens = tokenizer(prompt, return_tensors='pt').to(model.device)

# Generate text with parameters
generated_ids = model.generate(**tokens,
                                 max_new_tokens=22,  # Adjust as needed
                                 temperature=0.2,   # Adjust as needed
                                 top_k=33,         # Adjust as needed
                                 top_p=0.5,        # Adjust as needed
                                 do_sample=True)   # Enable sampling

# Decode the output
generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

# Print the result
print(generated_text)

!CUDA_VISIBLE_DEVICES=0 python generation_speed.py --model_name_or_path PATH/TO/MODEL/DIR

!git clone https://github.com/AutoGPTQ/AutoGPTQ.git

# Commented out IPython magic to ensure Python compatibility.
# %cd AutoGPTQ
!CUDA_VISIBLE_DEVICES=0 python /content/AutoGPTQ/examples/benchmark/generation_speed.py --model_name_or_path acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit

import torch
from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# --- الإعدادات الأساسية ---
# اسم المجلد الذي حفظت فيه النموذج المضغوط محليًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"
# أو اسم النموذج على Hugging Face Hub إذا قمت برفعه
# repo_id = "YourUserName/opt-125m-4bit"

# اسم المجلد أو المستودع الخاص بالنموذج الأصلي (لتحميل الـ Tokenizer)
# (عادة ما يكون هو نفسه الذي استخدمته للضغط)
pretrained_model_dir = "meta-llama/Meta-Llama-3-8B-Instruct"

# --- تحميل الـ Tokenizer ---
# تحتاج إلى Tokenizer لتحويل النص إلى مدخلات للنموذج والعكس
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)

# --- تحميل النموذج المضغوط ---
# اختر طريقة واحدة للتحميل:

# 1. التحميل من مجلد محلي:
logging.info(f"Loading quantized model from local directory: {quantized_model_dir}")
model = AutoGPTQForCausalLM.from_quantized(
    quantized_model_dir,
    device="cuda:0",  # تحديد الجهاز (e.g., "cuda:0" for first GPU, "cpu")
    use_safetensors=True # يفضل استخدامه إذا حفظت بهذا التنسيق
)
logging.info("Quantized model loaded successfully.")

# 2. (بديل) التحميل من Hugging Face Hub:
# قم بإزالة التعليق إذا كنت تريد التحميل من Hub واستبدل repo_id
# logging.info(f"Loading quantized model from Hub: {repo_id}")
# model = AutoGPTQForCausalLM.from_quantized(
#     repo_id,
#     device="cuda:0",
#     use_safetensors=True,
#     # use_triton=False # قد تحتاج لتعديل هذا بناءً على بيئتك وإمكانيات GPU
# )
# logging.info("Quantized model loaded successfully from Hub.")


# --- طرق الاستدلال (Inference) ---

# الطريقة 1: استخدام model.generate
logging.info("Performing inference using model.generate...")
prompt = "auto-gptq is"
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
output = model.generate(inputs=input_ids, max_new_tokens=50) # يمكنك تعديل max_new_tokens
generated_text_generate = tokenizer.decode(output[0])
print("--- Output from model.generate ---")
print(generated_text_generate)
print("-" * 30)


# الطريقة 2: استخدام TextGenerationPipeline (أسهل للاستخدام)
logging.info("Performing inference using TextGenerationPipeline...")
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
prompt_pipeline = "auto-gptq is"
result_pipeline = pipeline(prompt_pipeline, max_length=60) # يمكنك تعديل max_length
generated_text_pipeline = result_pipeline[0]["generated_text"]
print("--- Output from Pipeline ---")
print(generated_text_pipeline)
print("-" * 30)

!huggingface-cli login --token XX

# مثال مع model.generate
output = model.generate(
    inputs=input_ids,
    max_new_tokens=50,
    temperature=0.7,
    top_k=50,
    top_p=0.95,
    repetition_penalty=1.15
 )
print(tokenizer.decode(output[0]))

# مثال مع pipeline
result_pipeline = pipeline(
    prompt_pipeline,
    max_new_tokens=50, # أو استخدم max_length
    temperature=0.7,
    top_k=50,
    top_p=0.95,
    repetition_penalty=1.15
)
print(result_pipeline[0]["generated_text"])

import torch
from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# --- الإعدادات الأساسية ---
# اسم المجلد الذي حفظت فيه النموذج المضغوط محليًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"
# أو اسم النموذج على Hugging Face Hub إذا قمت برفعه
# repo_id = "YourUserName/opt-125m-4bit"

# اسم المجلد أو المستودع الخاص بالنموذج الأصلي (لتحميل الـ Tokenizer)
# (عادة ما يكون هو نفسه الذي استخدمته للضغط)
pretrained_model_dir = "meta-llama/Meta-Llama-3-8B-Instruct"

# --- تحميل الـ Tokenizer ---
# تحتاج إلى Tokenizer لتحويل النص إلى مدخلات للنموذج والعكس
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)

# --- تحميل النموذج المضغوط ---
# اختر طريقة واحدة للتحميل:

# 1. التحميل من مجلد محلي:
logging.info(f"Loading quantized model from local directory: {quantized_model_dir}")
model = AutoGPTQForCausalLM.from_quantized(
    quantized_model_dir,
    device="cuda:0",  # تحديد الجهاز (e.g., "cuda:0" for first GPU, "cpu")
    use_safetensors=True # يفضل استخدامه إذا حفظت بهذا التنسيق
)
logging.info("Quantized model loaded successfully.")

# 2. (بديل) التحميل من Hugging Face Hub:
# قم بإزالة التعليق إذا كنت تريد التحميل من Hub واستبدل repo_id
# logging.info(f"Loading quantized model from Hub: {repo_id}")
# model = AutoGPTQForCausalLM.from_quantized(
#     repo_id,
#     device="cuda:0",
#     use_safetensors=True,
#     # use_triton=False # قد تحتاج لتعديل هذا بناءً على بيئتك وإمكانيات GPU
# )
# logging.info("Quantized model loaded successfully from Hub.")


# --- طرق الاستدلال (Inference) ---

# الطريقة 1: استخدام model.generate
logging.info("Performing inference using model.generate...")
prompt = "auto-gptq is"
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
output = model.generate(inputs=input_ids, max_new_tokens=50) # يمكنك تعديل max_new_tokens
generated_text_generate = tokenizer.decode(output[0])
print("--- Output from model.generate ---")
print(generated_text_generate)
print("-" * 30)


# الطريقة 2: استخدام TextGenerationPipeline (أسهل للاستخدام)
logging.info("Performing inference using TextGenerationPipeline...")
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
prompt_pipeline = "auto-gptq is"
result_pipeline = pipeline(prompt_pipeline, max_length=60) # يمكنك تعديل max_length
generated_text_pipeline = result_pipeline[0]["generated_text"]
print("--- Output from Pipeline ---")
print(generated_text_pipeline)
print("-" * 30)

import torch
from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# --- الإعدادات الأساسية ---
# اسم المجلد الذي حفظت فيه النموذج المضغوط محليًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"
# أو اسم النموذج على Hugging Face Hub إذا قمت برفعه
# repo_id = "YourUserName/opt-125m-4bit"

# اسم المجلد أو المستودع الخاص بالنموذج الأصلي (لتحميل الـ Tokenizer)
# (عادة ما يكون هو نفسه الذي استخدمته للضغط)
pretrained_model_dir = "meta-llama/Meta-Llama-3-8B-Instruct"

# --- تحميل الـ Tokenizer ---
# تحتاج إلى Tokenizer لتحويل النص إلى مدخلات للنموذج والعكس
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)

# --- تحميل النموذج المضغوط ---
# اختر طريقة واحدة للتحميل:

# 1. التحميل من مجلد محلي:
logging.info(f"Loading quantized model from local directory: {quantized_model_dir}")
model = AutoGPTQForCausalLM.from_quantized(
    quantized_model_dir,
    device="cuda:0",  # تحديد الجهاز (e.g., "cuda:0" for first GPU, "cpu")
    use_safetensors=True # يفضل استخدامه إذا حفظت بهذا التنسيق
)
logging.info("Quantized model loaded successfully.")


logging.info("Performing inference using model.generate...")
prompt = "auto-gptq is"
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
output = model.generate(inputs=input_ids, max_new_tokens=50) # يمكنك تعديل max_new_tokens
generated_text_generate = tokenizer.decode(output[0])
print("--- Output from model.generate ---")
print(generated_text_generate)
print("-" * 30)

output = model.generate(
    inputs=input_ids,
    max_new_tokens=50,
    temperature=0.7,
    top_k=50,
    top_p=0.95,
    repetition_penalty=1.15
 )
print(tokenizer.decode(output[0]))



import torch
from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# --- الإعدادات الأساسية ---
# اسم المجلد الذي حفظت فيه النموذج المضغوط محليًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"
# أو اسم النموذج على Hugging Face Hub إذا قمت برفعه
# repo_id = "YourUserName/opt-125m-4bit"

# اسم المجلد أو المستودع الخاص بالنموذج الأصلي (لتحميل الـ Tokenizer)
# (عادة ما يكون هو نفسه الذي استخدمته للضغط)
pretrained_model_dir = "meta-llama/Meta-Llama-3-8B-Instruct"

# --- تحميل الـ Tokenizer ---
# تحتاج إلى Tokenizer لتحويل النص إلى مدخلات للنموذج والعكس
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)

# --- تحميل النموذج المضغوط ---
# اختر طريقة واحدة للتحميل:

# 1. التحميل من مجلد محلي:
logging.info(f"Loading quantized model from local directory: {quantized_model_dir}")
model = AutoGPTQForCausalLM.from_quantized(
    quantized_model_dir,
    device="cuda:0",  # تحديد الجهاز (e.g., "cuda:0" for first GPU, "cpu")
    use_safetensors=True # يفضل استخدامه إذا حفظت بهذا التنسيق
)
logging.info("Quantized model loaded successfully.")


logging.info("Performing inference using model.generate...")
prompt = "python is "
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
output = model.generate(
    inputs=input_ids,
    max_new_tokens=50,
    temperature=0.2,
    top_k=20,
    top_p=0.40,
    do_sample=True,
    repetition_penalty=1.15
 )
print(tokenizer.decode(output[0]))
generated_text_generate = tokenizer.decode(output[0])
print("--- Output from model.generate ---")
print(generated_text_generate)
print("-" * 30)

import torch
from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# --- الإعدادات الأساسية ---
# اسم المجلد الذي حفظت فيه النموذج المضغوط محليًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"
# أو اسم النموذج على Hugging Face Hub إذا قمت برفعه
# repo_id = "YourUserName/opt-125m-4bit"

# اسم المجلد أو المستودع الخاص بالنموذج الأصلي (لتحميل الـ Tokenizer)
# (عادة ما يكون هو نفسه الذي استخدمته للضغط)
pretrained_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"

# --- تحميل الـ Tokenizer ---
# تحتاج إلى Tokenizer لتحويل النص إلى مدخلات للنموذج والعكس
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)

# --- تحميل النموذج المضغوط ---
# اختر طريقة واحدة للتحميل:

# 1. التحميل من مجلد محلي:
logging.info(f"Loading quantized model from local directory: {quantized_model_dir}")
model = AutoGPTQForCausalLM.from_quantized(
    quantized_model_dir,
    device="cuda:0",  # تحديد الجهاز (e.g., "cuda:0" for first GPU, "cpu")
    use_safetensors=True # يفضل استخدامه إذا حفظت بهذا التنسيق
)
logging.info("Quantized model loaded successfully.")


logging.info("Performing inference using model.generate...")
prompt = "python is "
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
output = model.generate(
    inputs=input_ids,
    max_new_tokens=50,
    temperature=0.2,
    top_k=20,
    top_p=0.40,
    do_sample=True,
    repetition_penalty=1.15
 )
print(tokenizer.decode(output[0]))
generated_text_generate = tokenizer.decode(output[0])
print("--- Output from model.generate ---")
print(generated_text_generate)
print("-" * 30)

import torch
from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# --- الإعدادات الأساسية ---
# اسم المجلد الذي حفظت فيه النموذج المضغوط محليًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"
# أو اسم النموذج على Hugging Face Hub إذا قمت برفعه
# repo_id = "YourUserName/opt-125m-4bit"

# اسم المجلد أو المستودع الخاص بالنموذج الأصلي (لتحميل الـ Tokenizer)
# (عادة ما يكون هو نفسه الذي استخدمته للضغط)
pretrained_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"

# --- تحميل الـ Tokenizer ---
# تحتاج إلى Tokenizer لتحويل النص إلى مدخلات للنموذج والعكس
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)

# --- تحميل النموذج المضغوط ---
# اختر طريقة واحدة للتحميل:

# 1. التحميل من مجلد محلي:
logging.info(f"Loading quantized model from local directory: {quantized_model_dir}")
model = AutoGPTQForCausalLM.from_quantized(
    quantized_model_dir,
    device="cuda:0",  # تحديد الجهاز (e.g., "cuda:0" for first GPU, "cpu")
    use_safetensors=True # يفضل استخدامه إذا حفظت بهذا التنسيق
)
logging.info("Quantized model loaded successfully.")


logging.info("Performing inference using model.generate...")
prompt = "ai is "
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
output = model.generate(
    inputs=input_ids,
    max_new_tokens=50,
    temperature=0.1,
    top_k=10,
    top_p=0.20,
    do_sample=True,
    repetition_penalty=1
 )
print(tokenizer.decode(output[0]))
generated_text_generate = tokenizer.decode(output[0])
print("--- Output from model.generate ---")
print(generated_text_generate)
print("-" * 30)

import torch
from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# --- الإعدادات الأساسية ---
# اسم المجلد الذي حفظت فيه النموذج المضغوط محليًا
quantized_model_dir = "acl-srw-2024/openthai-7b-unsloth-gptq-2bit"
# أو اسم النموذج على Hugging Face Hub إذا قمت برفعه
# repo_id = "YourUserName/opt-125m-4bit"

# اسم المجلد أو المستودع الخاص بالنموذج الأصلي (لتحميل الـ Tokenizer)
# (عادة ما يكون هو نفسه الذي استخدمته للضغط)
pretrained_model_dir = "acl-srw-2024/openthai-7b-unsloth-gptq-2bit"

# --- تحميل الـ Tokenizer ---
# تحتاج إلى Tokenizer لتحويل النص إلى مدخلات للنموذج والعكس
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)

# --- تحميل النموذج المضغوط ---
# اختر طريقة واحدة للتحميل:

# 1. التحميل من مجلد محلي:
logging.info(f"Loading quantized model from local directory: {quantized_model_dir}")
model = AutoGPTQForCausalLM.from_quantized(
    quantized_model_dir,
    device="cuda:0",  # تحديد الجهاز (e.g., "cuda:0" for first GPU, "cpu")
    use_safetensors=True # يفضل استخدامه إذا حفظت بهذا التنسيق
)
logging.info("Quantized model loaded successfully.")


logging.info("Performing inference using model.generate...")
prompt = "python is "
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
output = model.generate(
    inputs=input_ids,
    max_new_tokens=50,
    temperature=0.2,
    top_k=20,
    top_p=0.40,
    do_sample=True,
    repetition_penalty=1.15
 )
print(tokenizer.decode(output[0]))
generated_text_generate = tokenizer.decode(output[0])
print("--- Output from model.generate ---")
print(generated_text_generate)
print("-" * 30)
print(tokenizer.decode(model.generate(**tokenizer("auto_gptq is", return_tensors="pt").to(model.device))[0]))

import torch
from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# --- الإعدادات الأساسية ---
# اسم المجلد الذي حفظت فيه النموذج المضغوط محليًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"
# أو اسم النموذج على Hugging Face Hub إذا قمت برفعه
# repo_id = "YourUserName/opt-125m-4bit"

# اسم المجلد أو المستودع الخاص بالنموذج الأصلي (لتحميل الـ Tokenizer)
# (عادة ما يكون هو نفسه الذي استخدمته للضغط)
pretrained_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"

# --- تحميل الـ Tokenizer ---
# تحتاج إلى Tokenizer لتحويل النص إلى مدخلات للنموذج والعكس
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)

# --- تحميل النموذج المضغوط ---
# اختر طريقة واحدة للتحميل:

# 1. التحميل من مجلد محلي:
logging.info(f"Loading quantized model from local directory: {quantized_model_dir}")
model = AutoGPTQForCausalLM.from_quantized(
    quantized_model_dir,
    device="cuda:0",  # تحديد الجهاز (e.g., "cuda:0" for first GPU, "cpu")
    use_safetensors=True # يفضل استخدامه إذا حفظت بهذا التنسيق
)
logging.info("Quantized model loaded successfully.")


logging.info("Performing inference using model.generate...")
prompt = "python is "
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
output = model.generate(
    inputs=input_ids,
    max_new_tokens=50,
    temperature=0.2,
    top_k=20,
    top_p=0.40,
    do_sample=True,
    repetition_penalty=1.15
 )
print(tokenizer.decode(output[0]))
generated_text_generate = tokenizer.decode(output[0])
print("--- Output from model.generate ---")
print(generated_text_generate)
print("-" * 30)
print(tokenizer.decode(model.generate(**tokenizer("auto_gptq is", return_tensors="pt").to(model.device))[0]))

from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

pretrained_model_dir = "meta-llama/Meta-Llama-3-8B-Instruct"
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"

tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)
examples = [
    tokenizer(
        "auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm."
    )
]

quantize_config = BaseQuantizeConfig(
    bits=4,  # quantize model to 4-bit
    group_size=128,  # it is recommended to set the value to 128
    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad
)

# load un-quantized model, by default, the model will always be loaded into CPU memory
model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)

# quantize model, the examples should be list of dict whose keys can only be "input_ids" and "attention_mask"
model.quantize(examples)

# save quantized model
model.save_quantized(quantized_model_dir)

# save quantized model using safetensors
model.save_quantized(quantized_model_dir, use_safetensors=True)

# push quantized model to Hugging Face Hub.
# to use use_auth_token=True, Login first via huggingface-cli login.
# or pass explcit token with: use_auth_token="hf_xxxxxxx"
# (uncomment the following three lines to enable this feature)
# repo_id = f"YourUserName/{quantized_model_dir}"
# commit_message = f"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}"
# model.push_to_hub(repo_id, commit_message=commit_message, use_auth_token=True)

# alternatively you can save and push at the same time
# (uncomment the following three lines to enable this feature)
# repo_id = f"YourUserName/{quantized_model_dir}"
# commit_message = f"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}"
# model.push_to_hub(repo_id, save_dir=quantized_model_dir, use_safetensors=True, commit_message=commit_message, use_auth_token=True)

# load quantized model to the first GPU
model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device="cuda:0")

# download quantized model from Hugging Face Hub and load to the first GPU
# model = AutoGPTQForCausalLM.from_quantized(repo_id, device="cuda:0", use_safetensors=True, use_triton=False)

# inference with model.generate
print(tokenizer.decode(model.generate(**tokenizer("auto_gptq is", return_tensors="pt").to(model.device))[0]))

# or you can also use pipeline
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
print(pipeline("auto-gptq is")[0]["generated_text"])

from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# مجلد النموذج المكمم مسبقًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"

# تحميل التوكنيزيشن من النموذج الأصلي
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct", use_fast=True)

# تحميل النموذج المكمم مباشرة على الـ GPU
model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device="cuda:0")

# اختبار التوليد باستخدام model.generate
print(tokenizer.decode(model.generate(**tokenizer("auto_gptq is", return_tensors="pt").to(model.device))[0]))

# أو استخدام pipeline
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
print(pipeline("auto-gptq is")[0]["generated_text"])

acl-srw-2024/openthai-7b-unsloth-gptq-2bit



from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# مجلد النموذج المكمم مسبقًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-3bit"

# تحميل التوكنيزيشن من النموذج الأصلي
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct", use_fast=True)

# تحميل النموذج المكمم مباشرة على الـ GPU
model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device="cuda:0")

# اختبار التوليد باستخدام model.generate
print(tokenizer.decode(model.generate(**tokenizer("auto_gptq is", return_tensors="pt").to(model.device))[0]))

# أو استخدام pipeline
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
print(pipeline("auto-gptq is")[0]["generated_text"])

from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# مجلد النموذج المكمم مسبقًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-2bit"

# تحميل التوكنيزيشن من النموذج الأصلي
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct", use_fast=True)

# تحميل النموذج المكمم مباشرة على الـ GPU
model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device="cuda:0")

# توليد النص باستخدام model.generate مع المعاملات المطلوبة
inputs = tokenizer("auto_gptq is", return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    do_sample=True,
    temperature=0.2,
    top_p=0.4,
    max_new_tokens=100
)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# أو استخدام pipeline مع نفس الإعدادات
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
print(pipeline(
    "auto-gptq is",
    do_sample=True,
    temperature=0.2,
    top_p=0.4,
    max_new_tokens=100
)[0]["generated_text"])

from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# مجلد النموذج المكمم مسبقًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-3bit"

# تحميل التوكنيزيشن من النموذج الأصلي
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct", use_fast=True)

# تحميل النموذج المكمم مباشرة على الـ GPU
model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device="cuda:0")

# توليد النص باستخدام model.generate مع المعاملات المطلوبة
inputs = tokenizer("auto_gptq is", return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    do_sample=True,
    temperature=0.2,
    top_p=0.4,
    max_new_tokens=100
)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# أو استخدام pipeline مع نفس الإعدادات
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
print(pipeline(
    "auto-gptq is",
    do_sample=True,
    temperature=0.2,
    top_p=0.4,
    max_new_tokens=100
)[0]["generated_text"])

from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# مجلد النموذج المكمم مسبقًا
quantized_model_dir = "acl-srw-2024/llama-3-8b-instruct-scb-gptq-3bit"

# تحميل التوكنيزيشن من النموذج الأصلي
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct", use_fast=True)

# تحميل النموذج المكمم مباشرة على الـ GPU
model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device="cuda:0")

# اختبار التوليد باستخدام model.generate
print(tokenizer.decode(model.generate(**tokenizer("what is python?", return_tensors="pt").to(model.device))[0]))

# أو استخدام pipeline
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
print(pipeline("what is python?")[0]["generated_text"])









!python /content/AutoGPTQ/examples/benchmark/generation_speed.py --model_name_or_path acl-srw-2024/llama-3-8b-instruct-scb-gptq-3bit



from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# مجلد النموذج المكمم مسبقًا
quantized_model_dir = "acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2"

# تحميل التوكنيزيشن من النموذج الأصلي
tokenizer = AutoTokenizer.from_pretrained("unsloth/Meta-Llama-3.1-8B-bnb-4bit", use_fast=True)

# تحميل النموذج المكمم مباشرة على الـ GPU
model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device="cuda:0")

# توليد النص باستخدام model.generate مع المعاملات المطلوبة
inputs = tokenizer("auto_gptq is", return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    do_sample=True,
    temperature=0.2,
    top_p=0.4,
    max_new_tokens=100
)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# أو استخدام pipeline مع نفس الإعدادات
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
print(pipeline(
    "auto-gptq is",
    do_sample=True,
    temperature=0.2,
    top_p=0.4,
    max_new_tokens=100
)[0]["generated_text"])

from transformers import AutoTokenizer, AutoModelForCausalLM, TextGenerationPipeline
import torch

model_id = "acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2"

tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.float16,
    load_in_4bit=True  # <-- هنا
)

# توليد النص
inputs = tokenizer("auto_gptq is", return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    do_sample=True,
    temperature=0.2,
    top_p=0.4,
    max_new_tokens=100
)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)

# مجلد النموذج المكمم
quantized_model_dir = "acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2"

# استخدم توكنيزيشن النموذج الأصلي المبني عليه (مثلاً نسخة LLaMA 3.1 من UnsLoTH)
tokenizer = AutoTokenizer.from_pretrained("unsloth/Meta-Llama-3.1-8B", use_fast=True)

# تحميل النموذج المكمم من مجلد GPTQ
model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device="cuda:0")

# توليد النص
inputs = tokenizer("auto_gptq is", return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    do_sample=True,
    temperature=0.2,
    top_p=0.4,
    max_new_tokens=100
)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# استخدام pipeline بنفس الإعدادات
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
print(pipeline(
    "auto-gptq is",
    do_sample=True,
    temperature=0.2,
    top_p=0.4,
    max_new_tokens=100
)[0]["generated_text"])

from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2"
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16,
    load_in_4bit=True
)

from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM
import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s",
    level=logging.INFO,
    datefmt="%Y-%m-%d %H:%M:%S"
)

# مجلد النموذج المكمم مسبقًا
quantized_model_dir = "acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2"

# تحميل التوكنيزيشن من نموذج LLama3 الأصلي (مطلوب لتجنب الخطأ)
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct", use_fast=True)

# تحميل النموذج المكمم مباشرة على الـ GPU
model = AutoGPTQForCausalLM.from_quantized(
    quantized_model_dir,
    device="cuda:0",
    use_safetensors=True,
    trust_remote_code=True
)

# توليد نصوص
inputs = tokenizer("auto_gptq is", return_tensors="pt").to(model.device)
outputs = model.generate(
    **inputs,
    do_sample=True,
    temperature=0.2,
    top_p=0.4,
    max_new_tokens=100
)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# عبر pipeline
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
print(pipeline(
    "auto-gptq is",
    do_sample=True,
    temperature=0.2,
    top_p=0.4,
    max_new_tokens=100
)[0]["generated_text"])

# Load model script
from auto_gptq import AutoGPTQForCausalLM
model = AutoGPTQForCausalLM.from_quantized("acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2", device="cuda:0",use_safetensors=True)



# Load model script
from auto_gptq import AutoGPTQForCausalLM
model = AutoGPTQForCausalLM.from_quantized("acl-srw-2024/llama3-8b-unsloth-sft-awq-8bit-v4", device="cuda:0",use_safetensors=True)



# Load model script
from auto_gptq import AutoGPTQForCausalLM
model = AutoGPTQForCausalLM.from_quantized("acl-srw-2024/llama3-8b-unsloth-sft-quip-2bit", device="cuda:0",use_safetensors=True)

# Load model script
from auto_gptq import AutoGPTQForCausalLM
# Either try acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2 which is quantized with GPTQ
# or acl-srw-2024/llama3-8b-unsloth-sft-awq-8bit-v4 which is quantized with AWQ.
model = AutoGPTQForCausalLM.from_quantized("acl-srw-2024/llama-8b-unsloth-sft-quip-2bit-pt-v2", device="cuda:0",use_safetensors=True)

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("acl-srw-2024/llama3-8b-unsloth-sft-awq-8bit-v4")
model = AutoModelForCausalLM.from_pretrained("acl-srw-2024/llama3-8b-unsloth-sft-awq-8bit-v4")

!pip install autoawq

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("acl-srw-2024/llama3-8b-unsloth-sft-awq-8bit-v4")
model = AutoModelForCausalLM.from_pretrained("acl-srw-2024/llama3-8b-unsloth-sft-awq-8bit-v4")

#!pip install autoawq # Make sure autoawq is installed

from transformers import AutoTokenizer, AutoModelForCausalLM

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("acl-srw-2024/llama3-8b-unsloth-sft-awq-8bit-v4")

# Load the model with AutoAWQForCausalLM instead of AutoModelForCausalLM
from autoawq import AutoAWQForCausalLM
model = AutoAWQForCausalLM.from_pretrained("acl-srw-2024/llama3-8b-unsloth-sft-awq-8bit-v4", device="cuda:0", trust_remote_code=True)

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# تحميل المعمارية الافتراضية لـ LLaMA 3
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3-8B")

# تحميل state_dict
state_dict = torch.load("model.pt", map_location="cpu")
model.load_state_dict(state_dict)

model.eval()

acl-srw-2024/mistral-7b-unsloth-sft-quip-2bit

from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "acl-srw-2024/mistral-7b-unsloth-sft-quip-2bit"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")